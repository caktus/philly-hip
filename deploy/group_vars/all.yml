---
# ----------------------------------------------------------------------------
# Global: Common configuration variables for all inventory groups
# ----------------------------------------------------------------------------

app_name: hip
# we use hip in some places and philly-hip in some places, so let's create a variable
long_app_name: "philly-{{ app_name }}"
aws_region: us-east-1
stack_name: "{{ long_app_name }}-stack"
aws_profile: "{{ long_app_name }}"
cluster_name: "{{ stack_name }}-cluster"

# ansible_connection: local
# ansible_python_interpreter: "{{ ansible_playbook_python }}"

k8s_cluster_name: "{{ cluster_name }}"
k8s_namespace: "{{ app_name }}-{{ env_name }}" 

# IP addresses
administrator_ip_cidrs:
  - 70.62.97.170/32  # Caktus office
  - 107.15.45.253/32  # Tobias
  - 47.230.11.28/32  # Ronard
  - 107.223.185.195/32  # Colin

# CloudFormation Outputs
# These values are taken from the CF 'Output' tab
# aws eks describe-cluster --name=philly-hip-stack-cluster | grep endpoint
ClusterEndpoint: https://C3219F3CB49E4B1C82CFE8C82A846345.sk1.us-east-1.eks.amazonaws.com
# aws rds describe-db-instances
DatabaseAddress: pd13w6wwn2hbn7f.cp7c2yqiusbp.us-east-1.rds.amazonaws.com
RepositoryURL: 061553509755.dkr.ecr.us-east-1.amazonaws.com/philly-hip-stack-applicationrepository-kk92mehevd86

# The RDS superuser password
admin_database_password: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  63666133363834643339373132356631656633343463313761376363613138383035353532346236
  3162396136333434303539346435306361336636636232620a353139306565616231303763646366
  31636366666262323933643061626135346663646564656534313437393063396633626332663831
  3565323636626163320a633963393664626563313265363632633161643833626366373265643835
  30383637393636336335303231653434666536623535313439646136663239383139323533613239
  6666643563326336613864366161623264363331656632333761

# ----------------------------------------------------------------------------
# caktus.aws-web-stacks: Ansible role to automate AWS CloudFormation stack
#                        provisioning with aws-web-stacks.
# ----------------------------------------------------------------------------
# Parameters
cloudformation_stack_database_engine_version: "15.5"
cloudformation_stack_database_parameter_group_family: postgres15

cloudformation_stack:
  profile: "{{ aws_profile }}"
  region: "{{ aws_region }}"
  stack_name: "{{ stack_name }}"
  template_bucket: "aws-web-stacks-{{ app_name }}"
  template_local_path: '{{ playbook_dir + "/stack/eks-nat.yml" }}'
  create_changeset: true
  termination_protection: true

  template_parameters:
    UseAES256Encryption: "true"
    AdministratorIPAddress: "{{ administrator_ip_cidrs[0] }}"  # Stack allows only single IP here to SSH to bastion
    BastionAMI: "ami-0ad554caf874569d2" # https://cloud-images.ubuntu.com/locator/ec2/ [us-east-1 amd64]
    BastionKeyName: rluna_hip
    CustomEKSAMI: "" # Optional when CustomAMIImageType is set. Preferrably do not set as AWS will select best optimized image if CustomAMIImageType is set.
    CustomAMIImageType: AL2023_x86_64_STANDARD
    BastionInstanceType: t3.small # Is this a proper size?
    BastionType: SSH
    CustomerManagedCmkArn: ""
    DomainName: "{{ app_name }}-prod.caktus-built.com"
    DomainNameAlternates: ""
    PrimaryAZ: "{{ aws_region }}a"
    SecondaryAZ: "{{ aws_region }}b"
    DesiredScale: 3
    MaxScale: 4
    ContainerInstanceType: t3a.large
    AssetsBucketAccessControl: Private
    AssetsUseCloudFront: "false"
    DatabaseClass: db.t3.small
    DatabaseEngineVersion: "{{ cloudformation_stack_database_engine_version }}"
    DatabaseParameterGroupFamily: "{{ cloudformation_stack_database_parameter_group_family }}"
    DatabaseName: "{{ app_name }}"
    DatabaseUser: "{{ app_name }}_admin"
    DatabasePassword: "{{ admin_database_password }}"
    DatabaseMultiAZ: "true"
    EksClusterName: "philly-hip-stack-cluster"
    EksPublicAccessCidrs: " {{ administrator_ip_cidrs | join(',') }}"
  tags:
    Environment: "{{ app_name }}"

# --------------------------------------------------------------------------
# caktus.k8s-web-cluster: Configuration variables for the single k8s cluster
# --------------------------------------------------------------------------

k8s_cluster_type: aws
# aws eks describe-cluster --name=philly-hip-stack-cluster --query 'cluster.arn'
k8s_context: "arn:aws:eks:us-east-1:061553509755:cluster/{{ cluster_name }}"
k8s_ingress_nginx_chart_version: "4.14.1"
k8s_cert_manager_chart_version: "v1.18"
k8s_letsencrypt_email: admin@caktusgroup.com
k8s_iam_users: [noop]  # https://github.com/caktus/ansible-role-k8s-web-cluster/issues/17
# aws-for-fluent-bit
# - https://github.com/aws/eks-charts/tree/master/stable/aws-for-fluent-bit
# - https://artifacthub.io/packages/helm/aws/aws-for-fluent-bit
k8s_aws_fluent_bit_chart_version: "0.1.32"
# aws-cloudwatch-metrics:
# - https://github.com/aws/eks-charts/tree/master/stable/aws-cloudwatch-metrics
# - https://artifacthub.io/packages/helm/aws/aws-cloudwatch-metrics
k8s_aws_cloudwatch_metrics_chart_version: "0.0.11"
k8s_aws_cloudwatch_metrics_namespace: amazon-cloudwatch

# ----------------------------------------------------------------------------
# caktus.django-k8s: Shared configuration variables for staging and production
#                    environments.
# ----------------------------------------------------------------------------

k8s_auth_host: "{{ ClusterEndpoint }}"
k8s_auth_ssl_ca_cert: "k8s_auth_ssl_ca_cert.txt"
k8s_memcached_enabled: true

# App pod configuration:
k8s_container_name: app
k8s_container_port: 8000
k8s_container_image: "{{ RepositoryURL }}"
k8s_container_image_pull_policy: Always
k8s_container_replicas: 2

# Lower resources to preserve Node resources
k8s_container_resources:
    requests:
        memory: "256Mi"
        cpu: "50m"

k8s_migrations_enabled: true
k8s_collectstatic_enabled: false
k8s_container_ingress_annotations:
  nginx.ingress.kubernetes.io/proxy-body-size: 100m
  # These are in seconds, but need to be specified without the trailing 's'
  # usually seen in nginx.conf proper.
  nginx.ingress.kubernetes.io/proxy-connect-timeout: 1800
  nginx.ingress.kubernetes.io/proxy-send-timeout: 1800
  nginx.ingress.kubernetes.io/proxy-read-timeout: 1800
# Shared environment variables:
env_database_url: "postgres://{{ app_name }}_{{ env_name }}:{{ database_password }}@{{ DatabaseAddress }}:5432/{{ app_name }}_{{ env_name }}"
env_django_settings: "{{ app_name }}.settings.deploy"
env_cache_host: memcached:11211
env_default_file_storage: "{{ app_name }}.storages.MediaBoto3Storage"
env_media_storage_bucket_name: "{{ app_name }}-{{ env_name }}-philly-private-assets"
env_aws_default_acl: ""
env_media_location: media/

# S3 bucket configuration:
k8s_s3_cluster_name: "{{ cluster_name }}"
k8s_s3_region: "{{ aws_region }}"
k8s_s3_public_bucket: "{{ k8s_s3_namespace }}-philly-assets"
k8s_s3_private_bucket: "{{ k8s_s3_namespace }}-philly-private-assets"

# Set up an AWS IAM user with limited perms for CI deploys
k8s_ci_username: hip-ci-user
k8s_ci_repository_arn: "arn:aws:ecr:us-east-1:061553509755:repository/philly-hip-stack-applicationrepository-kk92mehevd86"
k8s_ci_vault_password_arn: "arn:aws:secretsmanager:us-east-1:061553509755:secret:hip-ansible-vault-password-JYhbao"

# Email: 
env_email_host: email-smtp.us-east-1.amazonaws.com
env_email_use_tls: "true"
env_email_host_user: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  30613761343565386331633239623831303665313461356663393563346633373533316134633031
  3766633834376434363137646333666266353865343937360a613838306134663961333237393030
  39356265383036633765363635633232373066633639323763363935373934313632303830323964
  3265383761653137350a366134306338383537336336353266353439303539316334346330313439
  31666161613437643239373566303238353663653931343637353866303435666364
env_email_host_pass: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  62303635346364383964393536613631623730363337333337343930653030333865373539643736
  6138643665383165383863346239323066636233623937620a306137363539356362653935343338
  30366561316361633936613731333639373136323732616638313837633438343135323530623134
  6339646533356361340a633535323165653935376136303135353866353762663366663032376536
  32636365613634373961353564626336343930393866393130656666316634316431353431386330
  3561616461636134373033316665613035303736646133613630

# Azure SSO settings
azure_client_id: "39459c9f-77b3-4fae-942c-7ef9fbf1332c"
azure_tenant_id: "2046864f-68ea-497d-af34-a6629a6cd700"
azure_client_secret: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  66326536666564323330643461346239313738623566666633303934376132393739373862313463
  6331336437353664366661316631626262643031343736300a313963366537353861633330646463
  34396633643138313063326434323732343934383535636530646164323930356433666431396362
  3638303539323239380a333865663734336261343066313361343435376163326661366630633463
  31343965633836653436653465323438626262333565303034336237646665393661626362333766
  3130326232336638373737356261616530373138666465653137

k8s_environment_variables:
  DATABASE_URL: "{{ env_database_url }}"
  DJANGO_SETTINGS_MODULE: "{{ env_django_settings }}"
  DJANGO_DEBUG: "False"
  # DOMAIN is the ALLOWED_HOST
  DOMAIN: "{{ k8s_domain_names[0] }}"
  # join ALLOWED_HOSTS with a colon, because they are split by colon in deploy.py
  ALLOWED_HOSTS: "{{ k8s_domain_names|join(':') }}"
  ENVIRONMENT: "{{ env_name }}"
  CACHE_HOST: "{{ env_cache_host }}"
  # *** Uploaded media
  DEFAULT_FILE_STORAGE: "{{ env_default_file_storage }}"
  MEDIA_STORAGE_BUCKET_NAME: "{{ env_media_storage_bucket_name }}"
  AWS_DEFAULT_ACL: "{{ env_aws_default_acl }}"
  AWS_DEFAULT_REGION: "{{ aws_region }}"
  MEDIA_LOCATION: "{{ env_media_location }}"
  # *** Email
  EMAIL_HOST: "{{ env_email_host }}"
  EMAIL_HOST_USER: "{{ env_email_host_user }}"
  EMAIL_HOST_PASSWORD: "{{ env_email_host_pass }}"
  EMAIL_USE_TLS: "{{ env_email_use_tls }}"
  DJANGO_SECRET_KEY: "{{ env_django_secret_key }}"
  # Azure SSO settings
  AZURE_CLIENT_ID: "{{ azure_client_id }}"
  AZURE_TENANT_ID: "{{ azure_tenant_id }}"
  AZURE_CLIENT_SECRET: "{{ azure_client_secret }}"

# Install Descheduler to attempt to spread out pods again after node failures
k8s_install_descheduler: yes
# You must set the k8s_descheduler_chart_version to match the Kubernetes
# node version (0.23.x -> K8s 1.23.x); see:
# https://github.com/kubernetes-sigs/descheduler#compatibility-matrix
k8s_descheduler_chart_version: v0.33.0
# See values.yaml for options:
# https://github.com/kubernetes-sigs/descheduler/blob/master/charts/descheduler/values.yaml#L63
k8s_descheduler_release_values:
  deschedulerPolicy:
    strategies:
      # During upgrades or reboots, don't pre-emptively drain a node.
      RemovePodsViolatingNodeTaints:
        enabled: false
  

# Must be PostgreSQL v15 for DB restore
k8s_hosting_services_image_tag: 0.6.0-postgres15